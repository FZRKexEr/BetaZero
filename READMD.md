# BetaZero

## 核心模块

- **BoardGame**: 通用棋盘游戏接口，支持扩展到其他棋类游戏
- **Othello**: 高效的黑白棋实现，使用位运算和Zobrist哈希优化
- **MCTS**: 基础蒙特卡洛树搜索算法实现
- **NeuralNetworkMCTS**: 结合神经网络的MCTS实现
- **BatchNeuralNetwork**: 单个模型实例服务多个MCTS线程的批量推理优化
- **SelfPlay**: 多线程自我对弈数据生成
- **TrainingData**: 训练数据处理和增强
- **神经网络模型**: ResNet架构的价值网络和策略网络

## 训练流程

1. **模型更新策略**: 采用迭代自我更新策略，每轮训练后直接使用最新模型进行自我对弈
2. **数据生成**: 
   - 多线程并行自我对弈生成高质量训练数据
   - 使用任务级并行优化，多线程共享单个GPU上的神经网络模型
3. **模型训练**:
   - 使用TensorBoard监控训练过程
   - 支持数据增强，包括旋转、翻转和颜色变换
   - 混合精度训练提高效率

## MCTS实现细节

### 选择策略: N vs Q

经过实验分析，使用访问次数(N)作为选择策略比使用平均价值(Q)更有效：

- 更稳健，不易受单次评估异常的影响，Q使用均值容易受到异常值影响
- 更好地反映MCTS搜索过程的统计结果
- 生成的策略分布π能更好地代表真实的搜索偏好
- 综合考虑了探索和利用的平衡

### 随机性引入

训练的过程需要引入随机性来使得局面更加多样化，增加训练数据的多样性。

1. **Dirichlet噪声**: 在根节点的先验概率中引入Dirichlet噪声，增加探索性
2. **温度控制**: 前10手使用温度控制下一手的行动。后面使用贪婪策略保证结局能体现模型的能力。

### CPUCT参数分析

搜索公式中的CPUCT参数控制探索与利用的平衡：

- 随着N增多，`$\frac{\sqrt{\sum_b N(s_t, b)}}{1 + N(s_t, a)}$`会逐渐自动调整
- CPUCT的重要性会随着搜索深入而降低
- 推荐CPUCT参数范围为[2, 5]

## 性能优化

1. **并行策略**:
   - 任务级并行：多线程独立MCTS实例生成训练数据, 默认使用256线程
   - 推理服务器模式：所有线程共享单个GPU上的神经网络模型
   - 批量推理：合并多线程请求进行批量处理, 默认 batch_size=256

2. **算法优化**:
   - 黑白棋使用位运算实现高效棋盘表示
   - Zobrist哈希加速状态识别
   - 合法着法缓存减少重复计算

3. **性能指标**:
   - C++单线程纯MCTS搜索速度约12000节点/秒
   - 使用神经网络的MCTS比纯MCTS慢约10倍
   - 256线程批量推理模式下单次推理约7ms
